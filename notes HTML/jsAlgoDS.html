<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link rel="stylesheet" href="./dist/css/main.css" />
        <script src="./js/practice.js" defer></script>

        <title>Learning</title>
    </head>
    <body>
        <header>
            <h1>Learning</h1>
            <nav>
                <a href="index.html">Home</a>
                <a href="js.html">JS</a>
                <a href="node.html">Node & Packages</a>
                <a href="react.html">React</a>
                <a href="express.html">Express</a>
                <a href="mongo.html">MongoDB</a>
                <a href="css.html">CSS</a>
                <a href="linux.html">Linux</a>
                <a href="webserver.html">Web-Server</a>
                <a href="docker.html">Docker</a>
                <a href="ideas.html">Ideas</a>
                <a href="jsAlgoDS.html" class="active">JS Algo & DS</a>
                <a href="typescript.html">TypeScript</a>
            </nav>
        </header>
        <main>
            <h1> Javascript Algorithms & Data Structures </h1>

            <div class="topic">
                <ol>
                    <li>
                        Adding or removing any item from the starting of an
                        array consumes resources. It is because when we add or
                        remove item at the start, every item after it has to
                        change it's index number.</li
                    >
                </ol>
            </div>

            <div class="topic">
                <h2>Big <span class="highlight">O</span></h2>
                <p
                    >Big O Notation is a way to formalize fuzzy counting. Big O
                    notation is one of the most fundamental tools for computer
                    scientists to analyze the cost of an algorithm.</p
                ><p>
                    We say that an algorithm is O(f(n)) if the number of simple
                    operations the computer has to do is eventually less than a
                    constant times f(n), as n increases.
                    <br />
                    f(n) could be linear (f(n) = n)<br />
                    f(n) could be quadratic (f(n) = n )<br />
                    f(n) could be constant (f(n) = 1)<br />
                    f(n) could be something entirely different!<br />
                    <img
                        src="./img/jsAlgoDS/bigOExample.png"
                        alt="Big O Example"
                    />
                </p>
                <h3>Simplifying Big O Expressions</h3>
                <p>
                    <img
                        src="./img/jsAlgoDS/bigOGraphExample.png"
                        alt="Big O Graph"
                    />
                    <br />
                    When determining the time complexity of an algorithm, there
                    are some helpful rule of thumbs for big O expressions.
                    <br />
                    <br />
                    <strong> Constant time (O(1)) </strong>
                    <br />

                    O(1) does not change with respect to input space. Hence,
                    O(1) is referred to as being constant time.
                    <br />
                    <strong> Linear time (O(n)) </strong>
                    <br />

                    O(n) is linear time and applies to algorithms that must do n
                    operations in the worst-case scenario. most its just A
                    simple basic loop that within it we perform constant time
                    operations.
                    <br />
                    <strong> Logarithmic time O(log(n)) </strong>
                    <br />

                    A Logarithmic time function is one in which the time of
                    execution is proportional to the logarithm of the input
                    size.
                    <br />
                    <strong> Quadratic time(O(n^x )) </strong>
                    <br />

                    With quadratic time algorithms, we have now entered the dark
                    side of the time complexity.
                    <br />
                    As the name suggests, the size of the input quadratically
                    affects the running time of the algorithm. One common
                    example is nested loops
                    <br />
                </p>
                <br />
                <h3> Rules of Big-O Notation </h3>
                <p>
                    <strong> Coefficient Rule: “Get Rid of Constants” </strong>
                    <br />

                    It simply requires you to ignore any non-input-size-related
                    constants. Coefficients in Big-O are negligible with large
                    input sizes.
                    <br />
                    <strong> Sum Rule: “Add Big-Os Up” </strong>
                    <br />

                    The sum rule is intuitive to understand; time complexities
                    can be added.
                    <br />
                    <strong> Product Rule: “Multiply Big-Os” </strong>
                    <br />

                    The product rule simply states how Big-Os can be multiplied.
                    <br />
                    <strong>
                        Polynomial Rule: “Big-O to the Power of k”
                    </strong>
                    <br />

                    The polynomial rule states that polynomial time complexities
                    have a Big-O notation of the same polynomial degree.
                    <br />
                    If f(n) is a polynomial of degree k, then f(n) is
                    O(n<sup>k</sup>).
                    <br />
                </p>
            </div>
        </main>
    </body>
</html>
